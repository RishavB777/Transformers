{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 4\n",
    "batch_size = 1\n",
    "input_dim = 512\n",
    "d_model = 512 # Output of the attention unit for each word\n",
    "x = torch.randn((batch_size, sequence_length, input_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3291, -1.2081, -0.9383,  ..., -0.8130, -0.1673,  0.7833],\n",
       "         [ 1.0337,  0.0502, -0.6402,  ..., -0.4475, -0.5932, -0.3467],\n",
       "         [ 0.9311, -0.7564, -1.2766,  ...,  0.2758, -0.5427, -1.2825],\n",
       "         [-1.6686, -0.2311,  0.5470,  ..., -0.0551, -0.9282,  0.6740]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv_layer = nn.Linear(in_features=input_dim, out_features= 3 * d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv = qkv_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1536])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5394, -0.6173,  1.0741,  ...,  0.1133,  0.4265,  0.5257],\n",
       "         [-0.9898, -0.6422,  0.0614,  ...,  0.9905,  0.1241, -0.4120],\n",
       "         [ 0.2968,  0.3626, -1.2080,  ..., -0.8048, -0.7135,  0.3774],\n",
       "         [-1.1013,  0.1451, -0.1243,  ..., -0.4113,  0.5547,  0.2546]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'qkv distribution')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_val = torch.histc(qkv, bins=200, min=-3, max=3)\n",
    "x_val = np.arange(-1,1,0.01)*3\n",
    "plt.bar(x_val, y_val, align='center', color=['forestgreen'])\n",
    "plt.title(\"qkv distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "head_dim = d_model // num_heads\n",
    "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3* head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 192])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 192])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv.permute(0,2,1,3) # [batch_size, num_heads, sequence_length, 3*head_dim]\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q,k,v = qkv.chunk(3, dim=-1) # dim = -1 since we are breaking down the last dim into 3 parts\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention for multiple heads\n",
    "\n",
    "For a single head: \n",
    "![self-attention-formula.png](self-attention-formula.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "d_k = q.shape[-1]\n",
    "scaled = torch.matmul(q,k.transpose(-2,-1)) / math.sqrt(d_k) # (-2,-1) since we wanted to transpose the last 2 dimensions\n",
    "scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 64, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.transpose(-2,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.full(scaled.shape, float('-inf'))\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "mask[0][1] # mask for input to a single head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1988,    -inf,    -inf,    -inf],\n",
       "        [ 0.1615,  0.6117,    -inf,    -inf],\n",
       "        [-0.3072, -0.3834, -0.3972,    -inf],\n",
       "        [-0.3465, -0.4201,  0.2673, -0.2421]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(scaled + mask)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled+= mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38931321531114976"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(0.1615) / (np.exp(0.6117) + np.exp(0.1615))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "attention = F.softmax(scaled, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3893, 0.6107, 0.0000, 0.0000],\n",
       "        [0.3520, 0.3262, 0.3217, 0.0000],\n",
       "        [0.2046, 0.1901, 0.3781, 0.2272]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = torch.matmul(attention,v)\n",
    "values.shape # 0th dim -> Batch, 1st dim-> Heads, 2nd dim-> Sequence length, 3rd dim-> vectors per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def scaled_dot_product(q,k,v, mask=None):\n",
    "    d_k = q.shape[-1]\n",
    "    scaled = torch.matmul(q,k.transpose(-1,-2))/ math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled,dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, attention = scaled_dot_product(q,k,v, mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2633, 0.2269, 0.2523, 0.2576],\n",
       "        [0.2458, 0.3856, 0.1673, 0.2014],\n",
       "        [0.1964, 0.1820, 0.1795, 0.4421],\n",
       "        [0.2046, 0.1901, 0.3781, 0.2272]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = values.reshape(batch_size, sequence_length, num_heads * head_dim)\n",
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = linear_layer(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def scaled_dot_product(q,k,v, mask=None):\n",
    "    d_k = q.shape[-1]\n",
    "    scaled = torch.matmul(q,k.transpose(-1,-2))/ math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled,dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(input_dim, 3*d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, sequence_length, input_dim = x.shape\n",
    "        print(f\"x.shape:{x.shape}\")\n",
    "        qkv = self.qkv_layer(x)\n",
    "        print(f\"qkv.shape:{qkv.shape}\")\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3* self.head_dim)\n",
    "        print(f\"qkv.shape:{qkv.shape}\")\n",
    "        qkv = qkv.permute(0,2,1,3)\n",
    "        print(f\"qkv.shape:{qkv.shape}\")\n",
    "        q,k,v = qkv.chunk(3, dim=-1)\n",
    "        print(f\"q.size:{q.shape} | k.size:{k.shape} | v.size:{v.shape}\")\n",
    "        values, attention = scaled_dot_product(q,k,v,mask)\n",
    "        print(f\"values.shape:{values.shape} | attention.shape:{attention.shape}\")\n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads* self.head_dim)\n",
    "        print(f\"values.shape:{values.shape}\")\n",
    "        out = self.linear_layer(values)\n",
    "        print(f\"out.shape:{out.shape}\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:torch.Size([30, 5, 1024])\n",
      "qkv.shape:torch.Size([30, 5, 1536])\n",
      "qkv.shape:torch.Size([30, 5, 8, 192])\n",
      "qkv.shape:torch.Size([30, 8, 5, 192])\n",
      "q.size:torch.Size([30, 8, 5, 64]) | k.size:torch.Size([30, 8, 5, 64]) | v.size:torch.Size([30, 8, 5, 64])\n",
      "values.shape:torch.Size([30, 8, 5, 64]) | attention.shape:torch.Size([30, 8, 5, 5])\n",
      "values.shape:torch.Size([30, 5, 512])\n",
      "out.shape:torch.Size([30, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "input_dim = 1024\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "batch_size = 30\n",
    "sequence_length = 5\n",
    "x = torch.randn((batch_size, sequence_length, input_dim))\n",
    "\n",
    "model = MultiheadAttention(input_dim, d_model, num_heads)\n",
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
